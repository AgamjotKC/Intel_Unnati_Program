{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMER5qNwHx9RUjDHOXnOVDO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgamjotKC/Intel_Unnati_Program/blob/main/StorytellingBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing required libraries\n",
        "!pip install transformers\n",
        "\n",
        "# Importing necessary libraries from transformers and torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "rsIxi6S2Djox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the tokenizer and model with GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "6TzVwADzDjt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a story based on user-provided prompt\n",
        "def generate_story(prompt):\n",
        "    # Add a newline character to the prompt for better formatting\n",
        "    input_text = f\"{prompt}\\n\"\n",
        "\n",
        "    # Tokenize the input text to convert it into input IDs\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Create an attention mask (assuming all tokens should be attended to)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    # Set the maximum length for the generated story\n",
        "    max_length = 200\n",
        "\n",
        "    # Set the temperature for sampling (higher temperature means more randomness)\n",
        "    temperature = 0.8\n",
        "\n",
        "    # Generate the story using the model\n",
        "    output = model.generate(\n",
        "        input_ids,  # The tokenized input prompt\n",
        "        max_length=max_length,  # The maximum length of the generated story\n",
        "        num_return_sequences=1,  # Generate only one sequence\n",
        "        pad_token_id=tokenizer.eos_token_id,  # Use the EOS token for padding\n",
        "        attention_mask=attention_mask,  # Provide the attention mask\n",
        "        temperature=temperature,  # Use the specified temperature\n",
        "        do_sample=True  # Enable sampling instead of greedy decoding\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens to convert them back to a string\n",
        "    generated_story = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Return the generated story\n",
        "    return generated_story\n"
      ],
      "metadata": {
        "id": "v31W7h_bDjy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "prompt = \"Once upon a time in a faraway land\"\n",
        "story = generate_story(prompt)\n",
        "print(story)\n"
      ],
      "metadata": {
        "id": "6EO75swqDj2T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}