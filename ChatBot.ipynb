{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMPBHY5nCkZveCbPjuPUrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgamjotKC/Intel_Unnati_Program/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDadgSub3pfy"
      },
      "outputs": [],
      "source": [
        "#Installing the necessary libraries\n",
        "!pip install datasets transformers accelerate matplotlib -U\n",
        "\n",
        "# Importing necessary libraries\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import Dataset, load_metric\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, pipeline\n",
        "\n",
        "# Starting the timer to measure how long the whole process takes\n",
        "start_time = time.time()\n",
        "\n",
        "# Downloading the Alpaca dataset from GitHub\n",
        "!wget https://github.com/gururise/AlpacaDataCleaned/raw/main/alpaca_data_cleaned.json\n",
        "\n",
        "# Loading the dataset from the downloaded JSON file\n",
        "with open(\"alpaca_data_cleaned.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Initializing lists to store the input and target texts\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "# Extracting input and target texts from the dataset\n",
        "for entry in data:\n",
        "    if \"instruction\" in entry and \"input\" in entry and \"output\" in entry:\n",
        "        input_texts.append(entry[\"instruction\"] + \" \" + entry[\"input\"])\n",
        "        target_texts.append(entry[\"output\"])\n",
        "    else:\n",
        "        print(f\"Skipping entry due to missing fields: {entry}\")\n",
        "\n",
        "# Printing the total number of examples extracted\n",
        "print(f\"Total examples extracted: {len(input_texts)}\")\n",
        "\n",
        "# Creating a Hugging Face Dataset from the extracted texts\n",
        "dataset = Dataset.from_dict({\"input_text\": input_texts, \"target_text\": target_texts})\n",
        "\n",
        "# Initializing the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set the padding token to the end-of-sequence token\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    targets = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"labels\": targets[\"input_ids\"],\n",
        "    }\n",
        "\n",
        "# Tokenizing the dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
        "\n",
        "# Printing the total number of tokenized examples\n",
        "print(f\"Total tokenized examples: {len(tokenized_datasets)}\")\n",
        "\n",
        "# Splitting the tokenized dataset into training and validation sets (80% train, 20% validation)\n",
        "train_size = int(len(tokenized_datasets) * 0.8)\n",
        "train_dataset = tokenized_datasets.select(range(train_size))\n",
        "val_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
        "\n",
        "# Printing the number of examples in the training and validation sets\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(val_dataset)}\")\n",
        "\n",
        "# Visualizing the dataset distribution\n",
        "labels = ['Training', 'Validation']\n",
        "sizes = [len(train_dataset), len(val_dataset)]\n",
        "colors = ['skyblue', 'lightgreen']\n",
        "explode = (0.1, 0)  # explode the first slice\n",
        "\n",
        "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
        "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.title(\"Dataset Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Displaying samples from the dataset\n",
        "print(\"Sample training data (input -> output):\")\n",
        "for i in range(3):\n",
        "    print(f\"Input: {train_dataset[i]['input_ids'][:10]}... -> Output: {train_dataset[i]['labels'][:10]}...\")\n",
        "\n",
        "# Defining training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_steps=500,\n",
        "    warmup_steps=500,\n",
        "    save_total_limit=3,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "# Initializing the GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Initializing the Trainer with the model, training arguments, datasets, and tokenizer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "trainer.train()\n",
        "\n",
        "# Loading the BLEU metric for evaluation\n",
        "metric = load_metric(\"bleu\", trust_remote_code=True)\n",
        "\n",
        "# Initializing text generation pipeline with the trained model and tokenizer\n",
        "chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Printing the total time taken for the entire process\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(f\"Total time taken for the process: {total_time:.2f} seconds\")\n",
        "\n",
        "# Interactive loop to use the chatbot\n",
        "print(\"Hello! I'm your friendly AI chatbot. How can I assist you today?\")\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Chatbot: Goodbye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        # Generate a response from the chatbot\n",
        "        bot_response = chatbot(user_input, max_length=50, do_sample=True)[0]['generated_text']\n",
        "        print(\"Chatbot:\", bot_response)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ]
    }
  ]
}