{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJHWfgnoDdR38gVxllx/o6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgamjotKC/Intel_Unnati_Program/blob/main/FAQBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON6s-ioD4SQ4"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Log in to Hugging Face Hub\n",
        "login(token=\"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\")\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Load the FAQ dataset from Hugging Face\n",
        "faq_data = load_dataset(\"akshatshah1103/retail-faq\")\n",
        "print(faq_data['train'][:5])  # Print the first 5 examples from the training set\n",
        "\n",
        "# Combine the FAQ questions and responses into a single dataset\n",
        "combined_data = {\n",
        "    \"input\": faq_data['train']['FAQ'],\n",
        "    \"output\": faq_data['train']['Response']\n",
        "}\n",
        "combined_dataset = Dataset.from_dict(combined_data)\n",
        "print(combined_dataset[:5])  # Print the first 5 combined examples\n",
        "\n",
        "# Define the training arguments for the model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # Directory to save model checkpoints\n",
        "    per_device_train_batch_size=8,      # Batch size for training\n",
        "    num_train_epochs=3,                 # Number of training epochs\n",
        "    save_steps=5000,                    # Save model every 5000 steps\n",
        "    save_total_limit=1,                 # Keep only the latest model checkpoint\n",
        "    prediction_loss_only=True,          # Only calculate loss for prediction\n",
        "    logging_dir=\"./logs\",               # Directory to save logs\n",
        "    remove_unused_columns=False         # Keep all columns in the dataset\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the model, training arguments, and dataset\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=combined_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    trainer.train()\n",
        "except TypeError as e:\n",
        "    print(f\"Error during training: {e}\")\n",
        "\n",
        "# Define a chat function to generate responses\n",
        "def chat(prompt):\n",
        "    # Tokenize the input prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "    attention_mask = inputs.attention_mask.to(model.device)\n",
        "\n",
        "    # Generate a response using the model\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=150,  # Maximum length of the generated response\n",
        "        pad_token_id=tokenizer.eos_token_id,  # Padding token ID\n",
        "        num_return_sequences=1,  # Number of return sequences\n",
        "        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
        "        early_stopping=False,  # Do not stop early\n",
        "        num_beams=5,  # Number of beams for beam search\n",
        "        temperature=0.7  # Sampling temperature\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if prompt.lower() in response.lower():\n",
        "        response = response.replace(prompt.lower(), \"\").strip()\n",
        "    return response\n",
        "\n",
        "# Test the FAQ bot with some sample questions\n",
        "sample_questions = [\n",
        "    \"What is your return policy?\",\n",
        "    \"How do I track my order?\",\n",
        "    \"Do you offer international shipping?\"\n",
        "]\n",
        "\n",
        "# Generate and print responses for the sample questions\n",
        "for question in sample_questions:\n",
        "    answer = chat(question)\n",
        "    print(f\"Q: {question}\\nA: {answer}\\n\")\n",
        "\n"
      ]
    }
  ]
}